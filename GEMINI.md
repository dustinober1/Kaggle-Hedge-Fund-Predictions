# Project Context

## Overview
This project is for the Kaggle Hedge Fund Time Series Forecasting competition.

**Competition Details:**
- **Prize Pool**: $10,000 (1st: $3,500, 2nd: $2,500, 3rd: $2,000, 4th: $1,000, 5th: $1,000)
- **Evaluation**: Weighted RMSE Skill Score
- **Key Constraint**: No look-ahead - predictions for ts_index t can only use data from ts_index 0 to t

## Dataset Summary (From Exploration)
- **Training data**: 5,337,414 rows, 94 columns (~5.3 GB in memory)
- **Test data**: 1,447,107 rows, 92 columns (~1.4 GB in memory)
- **Features**: 86 anonymized features (feature_a to feature_ch)
- **Target column**: `y_target` (mean: -0.67, std: 32.53, range: -2201 to 2314)
- **Horizons**: 1 (short), 3 (medium), 10 (long), 25 (extra-long)

### Key Findings from Data Exploration

**Temporal Structure:**
- Train ts_index: 1 to 3601
- Test ts_index: 3602 to 4376
- **Zero overlap** - test data is strictly AFTER training data!

**Entity Structure:**
- 23 unique codes (all present in both train and test)
- 180 unique sub_codes in train, 47 in test (only 12 overlap)
- 5 sub_categories (all present in both)
- 9,270 unique entity combinations in train
- **2,299 NEW entity combinations in test** (not seen in training!)

**Target Variable (`y_target`):**
- Mostly centered around 0 (median: -0.0006)
- Very heavy tails (std=32.5, min=-2201, max=2314)
- Slightly negative bias overall (mean: -0.67)
- Longer horizons have more extreme values and higher std

**Weight Distribution:**
- Extremely skewed (mean: 16.4M, max: 13.9T)
- Weights only in training data (used for evaluation)
- Must use weighted loss functions

**Feature Insights:**
- 4 highly correlated feature pairs (|r| > 0.9):
  - feature_bm <-> feature_bo: 0.970
  - feature_u <-> feature_af: 0.956
  - feature_bz <-> feature_cd: 0.951
  - feature_ca <-> feature_cc: 0.942
- Top correlated with target: feature_bz (0.09), feature_cd (0.09)
- Missing values: feature_at (12.5%), feature_by (11%)

## Metric Analysis (Critical!)

**The competition metric is a SKILL SCORE:**
```
Score = sqrt(1 - min(max(ratio, 0), 1))
where ratio = sum(w*(y-pred)^2) / sum(w*y^2)
```

**Key Insight:** 
- Score = 0 means you're no better than predicting ZERO
- Score > 0 means predictions are CLOSER to true values than zero
- Predicting any constant other than 0 makes things MUCH WORSE

**Weight dominance:**
- Top 1% of samples contribute ~25% of denominator
- Top 10% of samples contribute ~72% of denominator
- Must focus on high-weight samples!

## Recent Changes
- Created Python virtual environment (`.venv`)
- Updated `.gitignore` to exclude `.venv`, `__pycache__`, etc.
- Added comprehensive `requirements.txt` with data science packages
- Created `src/01_data_exploration.py` for full data exploration
- Generated `outputs/exploration_results.json` with key statistics
- Created multiple LightGBM baselines (v1, v2, v3)
- Created `src/debug_metric.py` to analyze competition metric
- â­ Created `src/05_high_weight_focus.py` - tested 7 high-weight strategies
- â­ Created `src/06_strategy_refinement_v2.py` - **BREAKTHROUGH!** Beat zero baseline
- â­ Created `src/07_optimized_submission.py` - Final submission (score: 0.053)

## Current Best Score: 0.0529 ğŸ‰

**Winning Configuration:**
- Loss function: **Huber** (robust to outliers)
- Weight transformation: **sqrt(weight + 1)**
- Per-horizon models with horizon-specific shrinkage
- Shrinkage values: H1=0.12, H3=0.06, H10=0.27, H25=0.29

## Project Structure
```
â”œâ”€â”€ Competition_Rules/      # Competition rules and data description
â”‚   â”œâ”€â”€ overview.md
â”‚   â”œâ”€â”€ rules.md
â”‚   â””â”€â”€ data_descripition.md
â”œâ”€â”€ data/                   # Competition data
â”‚   â”œâ”€â”€ train.parquet
â”‚   â””â”€â”€ test.parquet
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ 01_data_exploration.py
â”‚   â”œâ”€â”€ 02_lgb_baseline.py
â”‚   â”œâ”€â”€ 03_lgb_baseline_v2.py
â”‚   â”œâ”€â”€ 04_lgb_baseline_v3.py
â”‚   â”œâ”€â”€ 05_high_weight_focus.py      # High-weight strategies
â”‚   â”œâ”€â”€ 06_strategy_refinement_v2.py # â­ BREAKTHROUGH strategies
â”‚   â”œâ”€â”€ 07_optimized_submission.py   # Final submission
â”‚   â””â”€â”€ debug_metric.py
â”œâ”€â”€ outputs/                # Generated outputs
â”‚   â”œâ”€â”€ exploration_results.json
â”‚   â”œâ”€â”€ submission_optimized_*.csv
â”‚   â””â”€â”€ *_results_*.json
â”œâ”€â”€ notebooks/              # Jupyter notebooks (to be added)
â”œâ”€â”€ .venv/                  # Python virtual environment
â”œâ”€â”€ requirements.txt        # Project dependencies
â”œâ”€â”€ REPORT.md              # Competition report
â””â”€â”€ GEMINI.md              # This file
```

## Next Steps
1. âœ… Run data exploration to understand the dataset
2. âœ… Build LightGBM baselines (all score 0 - need better approach)
3. âœ… Analyzed competition metric - need to beat zero predictions
4. âœ… **Focus on high-weight samples - SUCCESS! Score: 0.053**
5. Try feature engineering (lag features, rolling stats)
6. Try XGBoost/CatBoost as alternatives
7. Ensemble methods (blend Huber + Quantile models)

## Winning Strategy (PROVEN!)

### Key Discoveries from Experiments
1. **Shrinkage is critical**: Raw predictions are too noisy, must multiply by 0.1-0.3
2. **Huber loss beats MSE**: Robust to outliers in this noisy data
3. **Sqrt weights are optimal**: Neither raw nor log weights work as well
4. **Per-horizon models help**: Different horizons need different shrinkage values

### What Works
- âœ… **Huber loss** with sqrt(weight+1) transformation
- âœ… **Per-horizon models** with horizon-specific shrinkage
- âœ… **Shrinkage toward zero** (0.06 to 0.29 depending on horizon)
- âœ… **Simple LightGBM** with max_depth=6, num_leaves=31

### What Doesn't Work
- âŒ Using raw weights during training (too extreme)
- âŒ Predicting non-zero without shrinkage (ratio >> 1)
- âŒ Training only on high-weight samples (loses generalization)
- âŒ Importance sampling / oversampling

