{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedge Fund Time Series Forecasting - Data Exploration\n",
    "\n",
    "## Competition Overview\n",
    "This notebook performs a comprehensive exploration of the competition dataset to understand:\n",
    "- Data structure and types\n",
    "- Feature distributions and statistics\n",
    "- Missing values and data quality\n",
    "- Target variable characteristics\n",
    "- Temporal patterns\n",
    "- Category/entity relationships\n",
    "- Correlation analysis\n",
    "\n",
    "### Key Competition Details\n",
    "- **Evaluation Metric**: Weighted RMSE Skill Score\n",
    "- **Important Rule**: Predict `ts_index t` using only data from `ts_index 0 to t` (no look-ahead)\n",
    "- **Features**: 86 anonymized features (feature_a to feature_ch)\n",
    "- **Horizons**: 1 (short), 3 (medium), 10 (long), 25 (extra-long)\n",
    "- **Prize**: $10,000 total + potential job interview at hedge fund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet(DATA_DIR / 'train.parquet')\n",
    "test = pd.read_parquet(DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain memory usage: {train.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "print(f\"Test memory usage: {test.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN DATA - First 5 Rows\")\n",
    "print(\"=\" * 80)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test data\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST DATA - First 5 Rows\")\n",
    "print(\"=\" * 80)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column information\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN COLUMNS INFO\")\n",
    "print(\"=\" * 80)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "train_columns = train.columns.tolist()\n",
    "\n",
    "# Separate column groups\n",
    "id_cols = ['id']\n",
    "categorical_cols = ['code', 'sub_code', 'sub_category']\n",
    "temporal_cols = ['ts_index']\n",
    "horizon_col = ['horizon']\n",
    "weight_col = ['weight']\n",
    "target_col = ['target'] if 'target' in train_columns else []\n",
    "feature_cols = [col for col in train_columns if col.startswith('feature_')]\n",
    "\n",
    "print(f\"ID columns: {id_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Temporal columns: {temporal_cols}\")\n",
    "print(f\"Horizon column: {horizon_col}\")\n",
    "print(f\"Weight column: {weight_col}\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns (first 10): {feature_cols[:10]}\")\n",
    "print(f\"Feature columns (last 10): {feature_cols[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what columns are different between train and test\n",
    "train_cols_set = set(train.columns)\n",
    "test_cols_set = set(test.columns)\n",
    "\n",
    "print(\"Columns in train but not in test:\")\n",
    "print(train_cols_set - test_cols_set)\n",
    "\n",
    "print(\"\\nColumns in test but not in train:\")\n",
    "print(test_cols_set - train_cols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Categorical Variables Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of unique codes in train: {train['code'].nunique()}\")\n",
    "print(f\"Number of unique codes in test: {test['code'].nunique()}\")\n",
    "\n",
    "# Check overlap\n",
    "train_codes = set(train['code'].unique())\n",
    "test_codes = set(test['code'].unique())\n",
    "print(f\"\\nCodes only in train: {len(train_codes - test_codes)}\")\n",
    "print(f\"Codes only in test: {len(test_codes - train_codes)}\")\n",
    "print(f\"Codes in both: {len(train_codes & test_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-code analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"SUB_CODE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of unique sub_codes in train: {train['sub_code'].nunique()}\")\n",
    "print(f\"Number of unique sub_codes in test: {test['sub_code'].nunique()}\")\n",
    "\n",
    "# Check overlap\n",
    "train_subcodes = set(train['sub_code'].unique())\n",
    "test_subcodes = set(test['sub_code'].unique())\n",
    "print(f\"\\nSub_codes only in train: {len(train_subcodes - test_subcodes)}\")\n",
    "print(f\"Sub_codes only in test: {len(test_subcodes - train_subcodes)}\")\n",
    "print(f\"Sub_codes in both: {len(train_subcodes & test_subcodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-category analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"SUB_CATEGORY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of unique sub_categories in train: {train['sub_category'].nunique()}\")\n",
    "print(f\"Number of unique sub_categories in test: {test['sub_category'].nunique()}\")\n",
    "\n",
    "# Check overlap\n",
    "train_subcats = set(train['sub_category'].unique())\n",
    "test_subcats = set(test['sub_category'].unique())\n",
    "print(f\"\\nSub_categories only in train: {len(train_subcats - test_subcats)}\")\n",
    "print(f\"Sub_categories only in test: {len(test_subcats - train_subcats)}\")\n",
    "print(f\"Sub_categories in both: {len(train_subcats & test_subcats)}\")\n",
    "\n",
    "print(\"\\nAll sub_categories in train:\")\n",
    "print(sorted(train_subcats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Code distribution\n",
    "code_counts = train['code'].value_counts()\n",
    "axes[0].bar(range(len(code_counts)), code_counts.values)\n",
    "axes[0].set_title(f'Code Distribution (n={len(code_counts)})')\n",
    "axes[0].set_xlabel('Code Index')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Sub-code distribution\n",
    "subcode_counts = train['sub_code'].value_counts()\n",
    "axes[1].bar(range(len(subcode_counts)), subcode_counts.values)\n",
    "axes[1].set_title(f'Sub-code Distribution (n={len(subcode_counts)})')\n",
    "axes[1].set_xlabel('Sub-code Index')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Sub-category distribution\n",
    "subcat_counts = train['sub_category'].value_counts()\n",
    "axes[2].bar(range(len(subcat_counts)), subcat_counts.values)\n",
    "axes[2].set_title(f'Sub-category Distribution (n={len(subcat_counts)})')\n",
    "axes[2].set_xlabel('Sub-category Index')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizon distribution\n",
    "print(\"=\" * 80)\n",
    "print(\"HORIZON DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_horizon_counts = train['horizon'].value_counts().sort_index()\n",
    "test_horizon_counts = test['horizon'].value_counts().sort_index()\n",
    "\n",
    "axes[0].bar(train_horizon_counts.index.astype(str), train_horizon_counts.values, color='steelblue')\n",
    "axes[0].set_title('Train - Horizon Distribution')\n",
    "axes[0].set_xlabel('Horizon')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(train_horizon_counts.values):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "axes[1].bar(test_horizon_counts.index.astype(str), test_horizon_counts.values, color='coral')\n",
    "axes[1].set_title('Test - Horizon Distribution')\n",
    "axes[1].set_xlabel('Horizon')\n",
    "axes[1].set_ylabel('Count')\n",
    "for i, v in enumerate(test_horizon_counts.values):\n",
    "    axes[1].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTrain horizon value counts:\")\n",
    "print(train_horizon_counts)\n",
    "print(\"\\nTest horizon value counts:\")\n",
    "print(test_horizon_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis (ts_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_index analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"TS_INDEX (TEMPORAL) ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTrain ts_index range: {train['ts_index'].min()} to {train['ts_index'].max()}\")\n",
    "print(f\"Train ts_index unique values: {train['ts_index'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTest ts_index range: {test['ts_index'].min()} to {test['ts_index'].max()}\")\n",
    "print(f\"Test ts_index unique values: {test['ts_index'].nunique()}\")\n",
    "\n",
    "# Check for temporal overlap\n",
    "train_ts = set(train['ts_index'].unique())\n",
    "test_ts = set(test['ts_index'].unique())\n",
    "print(f\"\\nts_index overlap between train and test: {len(train_ts & test_ts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ts_index distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Train ts_index histogram\n",
    "axes[0, 0].hist(train['ts_index'], bins=100, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Train ts_index Distribution')\n",
    "axes[0, 0].set_xlabel('ts_index')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Test ts_index histogram\n",
    "axes[0, 1].hist(test['ts_index'], bins=100, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_title('Test ts_index Distribution')\n",
    "axes[0, 1].set_xlabel('ts_index')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Combined view\n",
    "axes[1, 0].hist(train['ts_index'], bins=100, color='steelblue', alpha=0.5, label='Train')\n",
    "axes[1, 0].hist(test['ts_index'], bins=100, color='coral', alpha=0.5, label='Test')\n",
    "axes[1, 0].set_title('Train vs Test ts_index Distribution')\n",
    "axes[1, 0].set_xlabel('ts_index')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Rows per ts_index\n",
    "train_ts_counts = train.groupby('ts_index').size()\n",
    "axes[1, 1].plot(train_ts_counts.index, train_ts_counts.values, alpha=0.7)\n",
    "axes[1, 1].set_title('Number of Rows per ts_index (Train)')\n",
    "axes[1, 1].set_xlabel('ts_index')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in train.columns:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nTarget Statistics:\")\n",
    "    print(train['target'].describe())\n",
    "    \n",
    "    print(f\"\\nTarget missing values: {train['target'].isna().sum()} ({100*train['target'].isna().mean():.2f}%)\")\n",
    "    print(f\"Target zeros: {(train['target'] == 0).sum()} ({100*(train['target'] == 0).mean():.2f}%)\")\n",
    "else:\n",
    "    print(\"No 'target' column found in training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in train.columns:\n",
    "    # Target distribution visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Raw distribution\n",
    "    axes[0, 0].hist(train['target'].dropna(), bins=100, color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Target Distribution (Raw)')\n",
    "    axes[0, 0].set_xlabel('Target')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Log-transformed distribution (if applicable)\n",
    "    target_positive = train.loc[train['target'] > 0, 'target']\n",
    "    if len(target_positive) > 0:\n",
    "        axes[0, 1].hist(np.log1p(target_positive), bins=100, color='seagreen', alpha=0.7)\n",
    "        axes[0, 1].set_title('Target Distribution (log1p, positive only)')\n",
    "        axes[0, 1].set_xlabel('log1p(Target)')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 2].boxplot(train['target'].dropna())\n",
    "    axes[0, 2].set_title('Target Box Plot')\n",
    "    axes[0, 2].set_ylabel('Target')\n",
    "    \n",
    "    # Target by horizon\n",
    "    train.boxplot(column='target', by='horizon', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Target by Horizon')\n",
    "    axes[1, 0].set_xlabel('Horizon')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # Target mean over time\n",
    "    target_by_ts = train.groupby('ts_index')['target'].mean()\n",
    "    axes[1, 1].plot(target_by_ts.index, target_by_ts.values, alpha=0.7)\n",
    "    axes[1, 1].set_title('Mean Target over ts_index')\n",
    "    axes[1, 1].set_xlabel('ts_index')\n",
    "    axes[1, 1].set_ylabel('Mean Target')\n",
    "    \n",
    "    # Target std over time\n",
    "    target_std_by_ts = train.groupby('ts_index')['target'].std()\n",
    "    axes[1, 2].plot(target_std_by_ts.index, target_std_by_ts.values, alpha=0.7, color='coral')\n",
    "    axes[1, 2].set_title('Std of Target over ts_index')\n",
    "    axes[1, 2].set_xlabel('ts_index')\n",
    "    axes[1, 2].set_ylabel('Std Target')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in train.columns:\n",
    "    # Target by category\n",
    "    print(\"\\nTarget Statistics by Sub-Category:\")\n",
    "    target_by_subcat = train.groupby('sub_category')['target'].agg(['mean', 'std', 'median', 'count'])\n",
    "    print(target_by_subcat.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"WEIGHT ANALYSIS (Important for evaluation!)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTrain Weight Statistics:\")\n",
    "print(train['weight'].describe())\n",
    "\n",
    "print(\"\\nTest Weight Statistics:\")\n",
    "print(test['weight'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight distribution visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Train weight distribution\n",
    "axes[0, 0].hist(train['weight'], bins=100, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Train Weight Distribution')\n",
    "axes[0, 0].set_xlabel('Weight')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Test weight distribution\n",
    "axes[0, 1].hist(test['weight'], bins=100, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_title('Test Weight Distribution')\n",
    "axes[0, 1].set_xlabel('Weight')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Weight by horizon\n",
    "train.boxplot(column='weight', by='horizon', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Weight by Horizon (Train)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Weight over time\n",
    "weight_by_ts = train.groupby('ts_index')['weight'].mean()\n",
    "axes[1, 1].plot(weight_by_ts.index, weight_by_ts.values, alpha=0.7)\n",
    "axes[1, 1].set_title('Mean Weight over ts_index')\n",
    "axes[1, 1].set_xlabel('ts_index')\n",
    "axes[1, 1].set_ylabel('Mean Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight patterns\n",
    "print(\"\\nWeight by Horizon:\")\n",
    "print(train.groupby('horizon')['weight'].describe())\n",
    "\n",
    "print(\"\\nWeight by Sub-Category:\")\n",
    "print(train.groupby('sub_category')['weight'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Feature statistics\n",
    "feature_stats = train[feature_cols].describe().T\n",
    "feature_stats['missing'] = train[feature_cols].isna().sum()\n",
    "feature_stats['missing_pct'] = 100 * train[feature_cols].isna().mean()\n",
    "feature_stats['zeros'] = (train[feature_cols] == 0).sum()\n",
    "feature_stats['zeros_pct'] = 100 * (train[feature_cols] == 0).mean()\n",
    "\n",
    "print(\"\\nFeature Statistics Summary:\")\n",
    "print(feature_stats.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"\\nFeatures with highest missing values:\")\n",
    "missing_features = feature_stats.sort_values('missing_pct', ascending=False)[['missing', 'missing_pct']]\n",
    "print(missing_features.head(20))\n",
    "\n",
    "# Visualize missing values\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "missing_pcts = train[feature_cols].isna().mean() * 100\n",
    "ax.bar(range(len(missing_pcts)), missing_pcts.values)\n",
    "ax.set_title('Missing Value Percentage by Feature')\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Missing %')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions (sample)\n",
    "sample_features = feature_cols[:9]  # First 9 features\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for idx, feat in enumerate(sample_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(train[feat].dropna(), bins=50, alpha=0.7)\n",
    "    ax.set_title(f'{feat}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Sample Feature Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Target correlations\n",
    "if 'target' in train.columns:\n",
    "    print(\"\\nFeature-Target Correlations (Pearson):\")\n",
    "    correlations = train[feature_cols + ['target']].corr()['target'].drop('target').sort_values(key=abs, ascending=False)\n",
    "    print(\"\\nTop 20 Most Correlated Features:\")\n",
    "    print(correlations.head(20))\n",
    "    print(\"\\nTop 20 Least Correlated Features:\")\n",
    "    print(correlations.tail(20))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    correlations_sorted = correlations.sort_values()\n",
    "    colors = ['coral' if x < 0 else 'steelblue' for x in correlations_sorted.values]\n",
    "    ax.bar(range(len(correlations_sorted)), correlations_sorted.values, color=colors)\n",
    "    ax.set_title('Feature-Target Correlations')\n",
    "    ax.set_xlabel('Feature (sorted by correlation)')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full feature correlation matrix (may be slow for 86 features)\n",
    "print(\"Computing feature correlation matrix...\")\n",
    "feature_corr = train[feature_cols].corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "upper_triangle = feature_corr.where(np.triu(np.ones(feature_corr.shape), k=1).astype(bool))\n",
    "high_corr_pairs = []\n",
    "for col in upper_triangle.columns:\n",
    "    for idx in upper_triangle.index:\n",
    "        corr_val = upper_triangle.loc[idx, col]\n",
    "        if pd.notna(corr_val) and abs(corr_val) > 0.9:\n",
    "            high_corr_pairs.append((idx, col, corr_val))\n",
    "\n",
    "print(f\"\\nHighly correlated feature pairs (|r| > 0.9): {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    for f1, f2, r in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:20]:\n",
    "        print(f\"  {f1} <-> {f2}: {r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (using clustering)\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.clustermap(feature_corr, cmap='RdBu_r', center=0, figsize=(16, 14),\n",
    "               dendrogram_ratio=0.1, cbar_pos=(0.02, 0.8, 0.03, 0.15))\n",
    "plt.suptitle('Feature Correlation Clustermap', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entity-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entity combinations\n",
    "print(\"=\" * 80)\n",
    "print(\"ENTITY-LEVEL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Unique entity combinations\n",
    "entity_combo_train = train.groupby(['code', 'sub_code', 'sub_category']).size().reset_index(name='count')\n",
    "entity_combo_test = test.groupby(['code', 'sub_code', 'sub_category']).size().reset_index(name='count')\n",
    "\n",
    "print(f\"Unique (code, sub_code, sub_category) combinations in train: {len(entity_combo_train)}\")\n",
    "print(f\"Unique (code, sub_code, sub_category) combinations in test: {len(entity_combo_test)}\")\n",
    "\n",
    "# Check overlap\n",
    "train_entities = set(zip(entity_combo_train['code'], entity_combo_train['sub_code'], entity_combo_train['sub_category']))\n",
    "test_entities = set(zip(entity_combo_test['code'], entity_combo_test['sub_code'], entity_combo_test['sub_category']))\n",
    "\n",
    "print(f\"\\nEntity combinations only in train: {len(train_entities - test_entities)}\")\n",
    "print(f\"Entity combinations only in test: {len(test_entities - train_entities)}\")\n",
    "print(f\"Entity combinations in both: {len(train_entities & test_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series length per entity\n",
    "entity_ts_length = train.groupby(['code', 'sub_code', 'sub_category'])['ts_index'].nunique().reset_index(name='ts_length')\n",
    "\n",
    "print(\"\\nTime series length per entity:\")\n",
    "print(entity_ts_length['ts_length'].describe())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(entity_ts_length['ts_length'], bins=50, alpha=0.7, color='steelblue')\n",
    "plt.title('Distribution of Time Series Length per Entity')\n",
    "plt.xlabel('Number of ts_index values')\n",
    "plt.ylabel('Count of entities')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Time Series Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample time series visualization\n",
    "if 'target' in train.columns:\n",
    "    # Get a sample entity\n",
    "    sample_entity = train.groupby(['code', 'sub_code', 'sub_category']).size().idxmax()\n",
    "    sample_data = train[(train['code'] == sample_entity[0]) & \n",
    "                        (train['sub_code'] == sample_entity[1]) &\n",
    "                        (train['sub_category'] == sample_entity[2])].sort_values('ts_index')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    for idx, horizon in enumerate(sample_data['horizon'].unique()[:4]):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        horizon_data = sample_data[sample_data['horizon'] == horizon]\n",
    "        ax.plot(horizon_data['ts_index'], horizon_data['target'], marker='o', alpha=0.7, markersize=2)\n",
    "        ax.set_title(f'Sample Entity - Horizon {horizon}')\n",
    "        ax.set_xlabel('ts_index')\n",
    "        ax.set_ylabel('Target')\n",
    "    \n",
    "    plt.suptitle(f'Sample Time Series: {sample_entity}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall missing values\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "missing_train = train.isna().sum()\n",
    "missing_test = test.isna().sum()\n",
    "\n",
    "for col in train.columns:\n",
    "    train_miss = missing_train[col]\n",
    "    if train_miss > 0:\n",
    "        print(f\"Train - {col}: {train_miss} ({100*train_miss/len(train):.2f}%)\")\n",
    "\n",
    "print(\"\")\n",
    "for col in test.columns:\n",
    "    test_miss = missing_test[col]\n",
    "    if test_miss > 0:\n",
    "        print(f\"Test - {col}: {test_miss} ({100*test_miss/len(test):.2f}%)\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(train.dtypes.value_counts())\n",
    "\n",
    "# Duplicates\n",
    "print(\"\\n--- Duplicate Checks ---\")\n",
    "print(f\"Duplicate IDs in train: {train['id'].duplicated().sum()}\")\n",
    "print(f\"Duplicate IDs in test: {test['id'].duplicated().sum()}\")\n",
    "print(f\"Duplicate rows in train (all columns): {train.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the data exploration, here are the key findings:\n",
    "\n",
    "1. DATA STRUCTURE:\n",
    "   - Training data: {} rows, {} columns\n",
    "   - Test data: {} rows, {} columns\n",
    "   - 86 anonymized features (feature_a to feature_ch)\n",
    "   - 4 forecast horizons: 1, 3, 10, 25\n",
    "\n",
    "2. TEMPORAL STRUCTURE:\n",
    "   - Train ts_index range: {} to {}\n",
    "   - Test ts_index range: {} to {}\n",
    "   - IMPORTANT: Test data comes from AFTER training data\n",
    "\n",
    "3. ENTITY STRUCTURE:\n",
    "   - Number of unique codes: {}\n",
    "   - Number of unique sub_codes: {}\n",
    "   - Number of unique sub_categories: {}\n",
    "\n",
    "4. RECOMMENDATIONS FOR MODELING:\n",
    "   - Use time-based validation (train on early ts_index, validate on later)\n",
    "   - Consider weighting recent data more heavily\n",
    "   - Build separate models or features for different horizons\n",
    "   - Handle entity hierarchies (code -> sub_code -> sub_category)\n",
    "   - Address missing values appropriately\n",
    "   - Use the weight column for weighted loss functions\n",
    "\n",
    "5. POTENTIAL APPROACHES:\n",
    "   - Gradient Boosting (LightGBM, XGBoost, CatBoost)\n",
    "   - Neural Networks with entity embeddings\n",
    "   - Time series specific models\n",
    "   - Ensemble methods\n",
    "\"\"\".format(\n",
    "    len(train), len(train.columns),\n",
    "    len(test), len(test.columns),\n",
    "    train['ts_index'].min(), train['ts_index'].max(),\n",
    "    test['ts_index'].min(), test['ts_index'].max(),\n",
    "    train['code'].nunique(),\n",
    "    train['sub_code'].nunique(),\n",
    "    train['sub_category'].nunique()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Exploration Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key statistics for later use\n",
    "import json\n",
    "\n",
    "exploration_results = {\n",
    "    'train_shape': train.shape,\n",
    "    'test_shape': test.shape,\n",
    "    'n_features': len(feature_cols),\n",
    "    'feature_cols': feature_cols,\n",
    "    'n_codes': train['code'].nunique(),\n",
    "    'n_sub_codes': train['sub_code'].nunique(),\n",
    "    'n_sub_categories': train['sub_category'].nunique(),\n",
    "    'horizons': sorted(train['horizon'].unique().tolist()),\n",
    "    'train_ts_range': [int(train['ts_index'].min()), int(train['ts_index'].max())],\n",
    "    'test_ts_range': [int(test['ts_index'].min()), int(test['ts_index'].max())],\n",
    "    'target_stats': train['target'].describe().to_dict() if 'target' in train.columns else None,\n",
    "}\n",
    "\n",
    "output_path = Path('../exploration_results.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(exploration_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Exploration results saved to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. **Feature Engineering**: Create time-based features, lag features, rolling statistics\n",
    "2. **Baseline Model**: Build a simple LightGBM baseline\n",
    "3. **Validation Strategy**: Implement proper time-based cross-validation\n",
    "4. **Advanced Models**: Try different architectures and ensembles\n",
    "5. **Hyperparameter Tuning**: Optimize model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
